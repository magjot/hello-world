{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgwkJGe6ESZK"
      },
      "source": [
        "# Multilingual ASR models with Subword Tokenization in NeMo\n",
        "This notebook helps you get started with NeMo multilingual ASR models; i.e. models that can transcribe audio in more than one language. You will learn how to use an existing pre-trained multilingual model for transcription, as well as how to create and train a new one.\n",
        "\n",
        "\n",
        "NOTE: User is responsible for checking the content of datasets and the applicable licenses and determining if suitable for the intended use.\n",
        "\n",
        "\n",
        "Please note that the current NeMo implementation is limited to models with subword tokenization.\n",
        "\n",
        "\n",
        "There are two general ways to create multilingual models.  One could simply combine datasets from different languages into one merged dataset and train a model on that dataset. For this to work, there's no need to even keep track of the language[s] in each sample. Nor would there be an easy way to determine which language was used during inference.\n",
        "\n",
        "\n",
        "The second approach, which we implemented in NeMo, centers around the idea of reusing existing pre-trained monolingual tokenizers for each language, and simply combining them. The tokenizers for each language could be trained on large non-ASR text corpora (for maximum generalization), extracted from existing monolingual NeMo checkpoints, or trained on monolingual ASR ground truth as usual.\n",
        "\n",
        "\n",
        "The diagram below illustrates how this works at inference time:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcF0yZQn110d"
      },
      "source": [
        "![Inference View](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/images/multilang_asr_inference.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyeYkFZBLS7b"
      },
      "source": [
        "We see that each language is assigned a range of token ids and the model is trained to produce token ids in that range in order to produce samples in that language. For instance, in the example in the diagram, if the token id generated by the model is 100, we know that it belongs to *lang1* and should be sent to *lang1* for decoding. If the token id is 1500, it belongs to *lang2*.  Note that we do not need to know which language a particular audio sample is in *a priori*; but the model generates an implicit language id for each token based on its id.\n",
        "\n",
        "The total number of classes the model is trained to output is simply the sum of the tokenizer vocabulary lengths.\n",
        "\n",
        "Note that we need to subtract the language offset number from each token id in order for the \"as is\" monolingual tokenizer to decode it.\n",
        "\n",
        "At training time, however, we do need to provide language id for each training sample. Here's how it works:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaErPULm110h"
      },
      "source": [
        "![Train View](https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/images/multilang_asr_train.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyv-mNlBNK8D"
      },
      "source": [
        "The audio for each sample is processed the same way as for the monolingual samples, but in order to generate the correct token ids for the transcripts, we need to know which tokenizer to use. Therefore, we do need an additional field ('lang') in the manifest for each sample.  After the monolingual tokenizer generates the token id, we offset it by the offset of each language and use it for model training.\n",
        "\n",
        "\n",
        "Now our picture is complete. Multilingual ground truth requires an extra field, 'lang' in the manifest for each sample and a separate monolingual tokenizer for each language.\n",
        "\n",
        "\n",
        "Let's see the code now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgBVmlQQ110j"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "%env DEBIAN_FRONTEND=noninteractive\n",
        "%env DEBCONF_NONINTERACTIVE_SEEN=true\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get update\n",
        "!apt-get -y install sox libsndfile1 ffmpeg\n",
        "!pip install text-unidecode\n",
        "!pip install matplotlib>=3.3.2\n",
        "# this is needed for RNNT loss\n",
        "!pip install --upgrade numba\n",
        "\n",
        "# this is needed to pre-process MCV Spanish dataset, which contains mp3 files\n",
        "!apt-get install -y sox libsox-fmt-mp3\n",
        "\n",
        "## Install NeMo\n",
        "## We are using the main branch but you might want to adjust that too\n",
        "BRANCH = 'r2.0.0rc0'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "\n",
        "\"\"\"\n",
        "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
        "Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n",
        "that you want to use the \"Run All Cells\" (or similar) option.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9SDDSwrWQt2"
      },
      "outputs": [],
      "source": [
        "import nemo.collections.asr as nemo_asr\n",
        "import os\n",
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMKgO-4RaUfp"
      },
      "source": [
        "## Datasets download\n",
        "Let's download the mini Librispeech (English) dataset. It is OK for the purposes of this tutorial, but for anything real, you will need to get at least the entire Librispeech dataset (960 hrs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vWr1vndadFR"
      },
      "outputs": [],
      "source": [
        "!mkdir -p datasets/mini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJB10XNUdLFT"
      },
      "source": [
        "We will use the `get_librispeech_data.py` script located in the nemo/scripts/dataset_processing dir if you cloned NeMo repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A2eK01CahPi"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"get_librispeech_data.py\"):\n",
        "    !wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/dataset_processing/get_librispeech_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIf1tDDZaySs"
      },
      "outputs": [],
      "source": [
        "!python get_librispeech_data.py \\\n",
        "  --data_root \"datasets/mini/\" \\\n",
        "  --data_sets mini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATf7lapcbAxB"
      },
      "source": [
        "Now, let's download the Mozilla CommonVoice Spanish dataset. We will ignore the larger train file and get just the test part for the purposes of this tutorial. For good results, you will need to get the train files and likely other datasets too, bringing the total to over 1k hours.\n",
        "\n",
        "Website steps:\n",
        "- Visit https://huggingface.co/settings/profile\n",
        "- Visit \"Access Tokens\" on list of items.\n",
        "- Create new token - provide a name for the token and \"read\" access is sufficient.\n",
        "  - PRESERVE THAT TOKEN API KEY. You can copy that key for next step.\n",
        "- Visit the [HuggingFace Dataset page for Mozilla Common Voice 3.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_3_0)\n",
        "  - There should be a section that asks you for your approval.\n",
        "  - Make sure you are logged in and then read that agreement.\n",
        "  - If and only if you agree to the text, then accept the terms.\n",
        "\n",
        "Code steps:\n",
        "- Now below, run `login()`\n",
        "- Paste your preserved HF TOKEN API KEY to the text box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVNuLvCbaydc"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpZNMYfKde9n"
      },
      "source": [
        "We will use the `convert_hf_dataset_to_nemo.py` script located in the nemo/scripts/speech_recognition dir if you cloned NeMo repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fdou8cubLZy"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"convert_hf_dataset_to_nemo.py\"):\n",
        "    !wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/speech_recognition/convert_hf_dataset_to_nemo.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJaihqWubtl4"
      },
      "outputs": [],
      "source": [
        "! python convert_hf_dataset_to_nemo.py \\\n",
        "    path=\"mozilla-foundation/common_voice_3_0\" \\\n",
        "    output_dir=\"datasets\" name=\"es\" split='test' \\\n",
        "    use_auth_token=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YO81u2tQtVP"
      },
      "source": [
        "To save time, let us create smaller subsets of the Spanish dataset for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFe1zmoUQsvA"
      },
      "outputs": [],
      "source": [
        "!head -1000 datasets/mozilla-foundation/common_voice_3_0/es/test/test_mozilla-foundation_common_voice_3_0_manifest.json > commonvoice_dev_manifest_1000.json\n",
        "!tail -1729 datasets/mozilla-foundation/common_voice_3_0/es/test/test_mozilla-foundation_common_voice_3_0_manifest.json > commonvoice_train_manifest.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB2JOsHs111C"
      },
      "outputs": [],
      "source": [
        "!wc -l commonvoice_train_manifest.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OugXEqVfcSqY"
      },
      "source": [
        "We will return to these datasets later for additional processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YX8Ur8GY6U8"
      },
      "source": [
        "# Inference using an existing multilingual checkpoint\n",
        "Pretrained Multilingual models can be worked with in the same way as monolingual ones. Let us start by downloading a pre-trained checkpoint from NGC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hcDCSunWYSJ"
      },
      "outputs": [],
      "source": [
        "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"stt_enes_contextnet_large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKq6EiPPZVty"
      },
      "source": [
        "Let's take a look at the tokenizer of this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxbs4ZbtEGAu"
      },
      "outputs": [],
      "source": [
        "print(OmegaConf.to_yaml(asr_model.cfg.tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32FOXBq-Yq6_"
      },
      "outputs": [],
      "source": [
        "print(OmegaConf.to_yaml(asr_model.tokenizer.langs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrrNjVIpZdll"
      },
      "source": [
        "We see that there are two tokenizers: English and Spanish, packaged inside the nemo tar archive. They are aggregated together using a special AggregateTokenizer class.\n",
        "\n",
        "In this case, there are two tokenizers, but you can have as many as you want.\n",
        "\n",
        "Let's transcribe some audio!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL701BkIZ3Z6"
      },
      "outputs": [],
      "source": [
        "en_files = [\"./datasets/mini/LibriSpeech/dev-clean-2-processed/7976-110523-0000.wav\", \"./datasets/mini/LibriSpeech/dev-clean-2-processed/7976-110523-0001.wav\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kit95pAHdpi_"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Load and listen to an english audio sample\n",
        "audio, sample_rate = librosa.load(en_files[1])\n",
        "\n",
        "ipd.Audio(audio, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhUh8bMnbGXC"
      },
      "outputs": [],
      "source": [
        "transcripts = asr_model.transcribe(audio = en_files) [0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk5FdZ5sd9yg"
      },
      "outputs": [],
      "source": [
        "transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtExsEMTbHnt"
      },
      "outputs": [],
      "source": [
        "es_files = ['datasets/mozilla-foundation/common_voice_3_0/es/test/clips/common_voice_es_18481930.wav', 'datasets/mozilla-foundation/common_voice_3_0/es/test/clips/common_voice_es_18481932.wav']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3gL5RTef8of"
      },
      "outputs": [],
      "source": [
        "# Now let's listen to a Spanish sample:\n",
        "audio, sample_rate = librosa.load(es_files[0])\n",
        "\n",
        "ipd.Audio(audio, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wspBidyabN5N"
      },
      "outputs": [],
      "source": [
        "asr_model.transcribe(audio = es_files) [0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLGSLdzDcHoV"
      },
      "source": [
        "We see that audio samples from both languages are transcribed well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA3OKDBAct_0"
      },
      "source": [
        "# Creating a new multilingual ASR model checkpoint from a monolingual one\n",
        "\n",
        "Training - or finetuning - a multilingual ASR model is no different to how you would do that with a monolingual model. The only difference is that you need to specify the language (id) for each sample in the manifest, e.g.\n",
        "```\n",
        "{\"audio_filepath\": \"/data/datasets/LibriSpeech/dev-clean-processed/422-122949-0002.wav\", \"duration\": 4.475, \"text\": \"we truthful ones the nobility in ancient greece called themselves\", \"lang\": \"en\"}\n",
        "```\n",
        "These language ids need to correspond to the language ids in your model's tokenizer config that we saw above, e.g.\n",
        "\n",
        "```\n",
        "tokenizer:\n",
        "  type:agg\n",
        "  langs:\n",
        "    en:\n",
        "      type: bpe\n",
        "      dir: en_tokenizer_dir\n",
        "    es:\n",
        "      type: bpe\n",
        "      dir: es_tokenizer_dir\n",
        "```\n",
        "As long as these are in place, training or fine tuning can proceed as usual.\n",
        "\n",
        "But, what if we are trying to create a brand new multilingual model and we want to start from a checkpoint created from a different language set?\n",
        "\n",
        "This works with all ASR models that use tokenization.\n",
        "\n",
        "In this notebook, we will use the small monolingual Conformer Transducer model checkpoint pre-trained on English:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE84E-AmhRQF"
      },
      "outputs": [],
      "source": [
        "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"stt_en_conformer_transducer_small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaIZtWELTe44"
      },
      "source": [
        "We will need to change the tokenizer for our loaded checkpoint to a multilingual one. You also could simply transfer an already trained tokenizer from another nemo model - every .nemo archive has the tokenizer (or more than one tokenizer) packaged inside.\n",
        "\n",
        "You also could train the tokenizer on any NLP ground truth - not necessarily ASR ground truth, potentially leading to better results.  \n",
        "\n",
        "But the *simplest* way of doing this would be to train a tokenizer on our datasets at hand.  Let's see how we can do that:  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIDFDQqBjnS7"
      },
      "source": [
        "Let's download the tokenizer creation script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMpefBZn1reO"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"process_asr_text_tokenizer.py\"):\n",
        "  !wget  https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tokenizers/process_asr_text_tokenizer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klk6Bmmm4ZND"
      },
      "source": [
        "Now let's create / train our tokenizers on our training manifests. You will notice later that we will train the model on the \"test\" manifests and validate it against the \"dev\" manifests - just to shorten training time. We will also train our Spanish tokenizer on the entire test manifest, not limiting it to the first 1500 lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQMgsA4r3Vnx"
      },
      "outputs": [],
      "source": [
        "ENGLISH_TOKENIZER_DIR = \"tokenizers/en\"\n",
        "SPANISH_TOKENIZER_DIR = \"tokenizers/es\"\n",
        "!mkdir -p $ENGLISH_TOKENIZER_DIR\n",
        "!mkdir -p $SPANISH_TOKENIZER_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLW3QSYs111J"
      },
      "source": [
        "We will use the entire Spanish test manifest to train our tokenizer to improve results. The more data is usually the better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5SRfudY3FH3"
      },
      "outputs": [],
      "source": [
        "!python process_asr_text_tokenizer.py \\\n",
        "  --manifest=\"commonvoice_train_manifest.json\" \\\n",
        "  --data_root=$SPANISH_TOKENIZER_DIR \\\n",
        "  --vocab_size=128 \\\n",
        "  --tokenizer=\"spe\" \\\n",
        "  --spe_type=bpe \\\n",
        "  --spe_character_coverage=1.0 \\\n",
        "  --log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDtKNwLb5NCc"
      },
      "outputs": [],
      "source": [
        "!python process_asr_text_tokenizer.py \\\n",
        "  --manifest=\"datasets/mini/train_clean_5.json\" \\\n",
        "  --data_root=$ENGLISH_TOKENIZER_DIR \\\n",
        "  --vocab_size=128 \\\n",
        "  --tokenizer=\"spe\" \\\n",
        "  --spe_type=bpe \\\n",
        "  --spe_character_coverage=1.0 \\\n",
        "  --log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DISSpZGm5qwd"
      },
      "outputs": [],
      "source": [
        "!ls $ENGLISH_TOKENIZER_DIR/tokenizer_spe_bpe_v128\n",
        "!ls $SPANISH_TOKENIZER_DIR/tokenizer_spe_bpe_v128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5B9g_l5-c9i"
      },
      "source": [
        "All good. Let's get a Spanish tokenizer from a Spanish model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcR3ThUJ_Cxj"
      },
      "source": [
        "Now we can create a config for our new aggregate Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCXjT2k-G9jx"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "new_tokenizer_cfg = OmegaConf.create({'type': 'agg', 'langs': {}})\n",
        "english_tokenizer_cfg = OmegaConf.create({'dir': ENGLISH_TOKENIZER_DIR + '/tokenizer_spe_bpe_v128', 'type': 'bpe'})\n",
        "spanish_tokenizer_cfg = OmegaConf.create({'dir': SPANISH_TOKENIZER_DIR + '/tokenizer_spe_bpe_v128', 'type': 'bpe'})\n",
        "new_tokenizer_cfg.langs['en'] = english_tokenizer_cfg\n",
        "new_tokenizer_cfg.langs['es'] = spanish_tokenizer_cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrC04Kdo_Lzy"
      },
      "source": [
        "And apply it to our loaded checkpoint.  Note that this completely replaces the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL68UJq_icuW"
      },
      "outputs": [],
      "source": [
        "asr_model.change_vocabulary(\n",
        "        new_tokenizer_dir=new_tokenizer_cfg,\n",
        "        new_tokenizer_type=\"agg\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRU2aofs111N"
      },
      "source": [
        "You may notice that some of the tokens contain characters that are neither part of the English nor Spanish alphabet. This is because the official Mozilla Common Voice data for each language sometimes contains a few utterances from a different language due to some data processing errors. Ideally we would remove these other-language utterances before training. For the purposes of this tutorial, we have skipped this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrimSD1EjulS"
      },
      "source": [
        "The tokenizer has been changed, let's save our new checkpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3g1URYoj03N"
      },
      "outputs": [],
      "source": [
        "asr_model.save_to(\"multi.nemo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbQ5kGYdl3Dw"
      },
      "source": [
        "Now, let's load this new checkpoint and make sure the tokenizer config looks ok:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1fcl41glj6F"
      },
      "outputs": [],
      "source": [
        "asr_model = nemo_asr.models.EncDecRNNTBPEModel.restore_from(restore_path=\"multi.nemo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXq2CRYBlsqc"
      },
      "outputs": [],
      "source": [
        "print(asr_model.cfg.tokenizer)\n",
        "print(asr_model.tokenizer.langs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AltE_vEzmD0a"
      },
      "source": [
        "This new checkpoint has encoder weights from the English model, but the decoder has been reinitialized. Let's see how well it does on English speech:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S2GczoimDBg"
      },
      "outputs": [],
      "source": [
        "asr_model.transcribe(audio = en_files)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vmoGxZAmjU_"
      },
      "source": [
        "How about Spanish?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T882odxtmlRO"
      },
      "outputs": [],
      "source": [
        "asr_model.transcribe(audio = es_files)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPVsG7jAmpUA"
      },
      "source": [
        "That didn't work. The decoder + joint weights are now larger, and they don't correspond to the original weights. Also, the RNNT blank token is now shared between the two languages, not duplicated at the end of each languages range. The model needs to be fine-tuned before we can use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F49I_1MO4kje"
      },
      "source": [
        "## Creating training / validation sets\n",
        "We previously downloaded an English and Spanish dataset, but we will need to make adjustments in order to use them for training and eval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9c8rUJm6S1r"
      },
      "source": [
        "Let us take a look at the preprocessed mini librispeech manifest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76bxoKpI5nUD"
      },
      "outputs": [],
      "source": [
        "train_manifest_en_in = \"datasets/mini/train_clean_5.json\"\n",
        "val_manifest_en_in = \"datasets/mini/dev_clean_2.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuKXK-1n5qh6"
      },
      "outputs": [],
      "source": [
        "!head -5 $train_manifest_en_in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTrvk40_6jgW"
      },
      "source": [
        "As described above, multilingual models require the language id field for each sample. This manifest is all english, so it's easy to add the lang:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNMreApz61tB"
      },
      "outputs": [],
      "source": [
        "train_manifest_en = \"datasets/mini/train_clean_5_en.json\"\n",
        "val_manifest_en = \"datasets/mini/dev_clean_2_en.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AL1VXcv_0Fu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def add_lang(in_manifest_file, out_manifest_file, lang):\n",
        "    with open(in_manifest_file) as in_file:\n",
        "        with open(out_manifest_file, 'w') as out_file:\n",
        "            for line in in_file:\n",
        "                o = json.loads(line)\n",
        "                o['lang'] = lang\n",
        "                s = json.dumps(o)\n",
        "                out_file.write(s + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAKZR4CEEIx2"
      },
      "outputs": [],
      "source": [
        "add_lang(train_manifest_en_in, train_manifest_en, 'en')\n",
        "add_lang(val_manifest_en_in, val_manifest_en, 'en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8EOOhy0E5So"
      },
      "source": [
        "Same needs to be done to Spanish manifests. Note that we will use the entire Spanish test manifest for training in order to improve results - even though this creates an imbalanced training set. You can try to use the smaller (1500) manifest too for quicker training, but the results will not be as good. We can still keep the Spanish validation set small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxAuCaJkEV_h"
      },
      "outputs": [],
      "source": [
        "train_manifest_es_in = \"commonvoice_train_manifest.json\"\n",
        "val_manifest_es_in = \"commonvoice_dev_manifest_1000.json\"\n",
        "train_manifest_es = \"commonvoice_train_manifest_es.json\"\n",
        "val_manifest_es = \"commonvoice_dev_manifest_1000_es.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uum4VPiYEu_F"
      },
      "outputs": [],
      "source": [
        "add_lang(train_manifest_es_in, train_manifest_es, 'es')\n",
        "add_lang(val_manifest_es_in, val_manifest_es, 'es')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgFe5bIqACOJ"
      },
      "source": [
        "Let's check that the lang fields were added:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GazCYyGN_4ZH"
      },
      "outputs": [],
      "source": [
        "!head -3 $train_manifest_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjCxI8g3FBbZ"
      },
      "outputs": [],
      "source": [
        "!head -3 $train_manifest_es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To0bBZOmSKZ6"
      },
      "source": [
        "Great! Let us initialize the PyTorch Lightning trainer. Note that we are using native 16-bit precision. Please change this to 32 bit if your model shows any signs of divergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HidMfcldAfBf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as ptl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXG-Ogf111T"
      },
      "source": [
        "##### Initializing the trainer\n",
        "We will run for just 5 epochs to save time, even though you will likely see better results at about 10-15. You may want to increase gradient accumulation if the loss oscillates too much.  If your machine has more than one GPU, add them to the GPUs list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOh4YQHf111T"
      },
      "outputs": [],
      "source": [
        "GRAD_ACCUM=1\n",
        "MAX_EPOCHS=5\n",
        "GPUS=[0]\n",
        "LOG_EVERY_N_STEPS=10\n",
        "\n",
        "trainer = ptl.Trainer(devices=GPUS,\n",
        "                      accelerator=\"gpu\",\n",
        "                      max_epochs=MAX_EPOCHS,\n",
        "                      accumulate_grad_batches=GRAD_ACCUM,\n",
        "                      precision=16,\n",
        "                      enable_checkpointing=False,\n",
        "                      logger=False,\n",
        "                      log_every_n_steps=LOG_EVERY_N_STEPS,\n",
        "                      enable_progress_bar=True,\n",
        "                      check_val_every_n_epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA-1hFkN111U"
      },
      "outputs": [],
      "source": [
        "asr_model.set_trainer(trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIwzcmsRSdO0"
      },
      "source": [
        "Now the training dataloader. Observe that we will be training on a mix of spanish and english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p2GUoCIBo2v"
      },
      "outputs": [],
      "source": [
        "train_ds = {}\n",
        "train_ds['manifest_filepath'] = [train_manifest_en,train_manifest_es]\n",
        "train_ds['sample_rate'] = 16000\n",
        "train_ds['batch_size'] = 16\n",
        "train_ds['fused_batch_size'] = 16\n",
        "train_ds['shuffle'] = True\n",
        "train_ds['max_duration'] = 16.7\n",
        "train_ds['pin_memory'] = True\n",
        "train_ds['is_tarred'] = False\n",
        "train_ds['num_workers'] = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPrK7MvvFDIh"
      },
      "outputs": [],
      "source": [
        "asr_model.setup_training_data(train_data_config=train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3p46a8VSnH2"
      },
      "source": [
        "For the validation dataset, we follow a similar process. The batch size for validation can be higher since memory pressure is lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdTa46atCnpr"
      },
      "outputs": [],
      "source": [
        "validation_ds = {}\n",
        "validation_ds['sample_rate'] = 16000\n",
        "validation_ds['manifest_filepath'] = [val_manifest_en,val_manifest_es]\n",
        "validation_ds['batch_size'] = 32\n",
        "validation_ds['shuffle'] = False\n",
        "validation_ds['num_workers'] = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is0KS6Od111W"
      },
      "source": [
        "We will use the `setup_multiple_validation_data` call to see validation numbers for English and Spanish datasets separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrfDhIenFE9B"
      },
      "outputs": [],
      "source": [
        "asr_model.setup_multiple_validation_data(val_data_config=validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n06REaHQhvf-"
      },
      "source": [
        "Now, let us tweak the model optimizer parameters. The model was pre-trained with a higher learning rate; we may want to lower it a little for fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWfQhGeBh7SF"
      },
      "outputs": [],
      "source": [
        "optimizer_conf = {}\n",
        "\n",
        "optimizer_conf['name'] = 'adamw'\n",
        "optimizer_conf['lr'] = 0.01\n",
        "optimizer_conf['betas'] =  [0.9, 0.98]\n",
        "optimizer_conf['weight_decay'] = 0\n",
        "\n",
        "sched = {}\n",
        "sched['name'] = 'CosineAnnealing'\n",
        "sched['warmup_steps'] = None\n",
        "sched['warmup_ratio'] = 0.10\n",
        "sched['min_lr'] = 1e-6\n",
        "optimizer_conf['sched'] = sched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaFInkSs111X"
      },
      "outputs": [],
      "source": [
        "asr_model.setup_optimization(optimizer_conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQpvI8IliD66"
      },
      "source": [
        "Let us freeze the encoder for easier initial convergence and faster training. On a smaller dataset when retraining the decoder, this is often a good idea.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H5P2u8FJ4M8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def enable_bn_se(m):\n",
        "    if type(m) == nn.BatchNorm1d:\n",
        "        m.train()\n",
        "        for param in m.parameters():\n",
        "            param.requires_grad_(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgdrz1kNJ5A8"
      },
      "outputs": [],
      "source": [
        "asr_model.encoder.freeze()\n",
        "asr_model.encoder.apply(enable_bn_se)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "QkMlRsku111Y"
      },
      "source": [
        "Let is enable the logging of the validation loss and suppress the printing of log predictions during training, in order to not pollute the Jupiter notebook output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "H9FXDTRK111Z"
      },
      "outputs": [],
      "source": [
        "asr_model.wer.log_predictions = False\n",
        "# set to True if you would like to track the evaluation loss\n",
        "asr_model.compute_eval_loss = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIjvGFy6kzPy"
      },
      "source": [
        "Finally, let's set up an experiment manager to keep track of our checkpoints.  Also, we want to use W&B for experiment management!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDlwk1D1lBo_"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrhCRhDC111c"
      },
      "source": [
        "Since we will use Weights and Biases to observe our run, you will need an account.  You will also need to rename the project name and experiment name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3rdM7gx111c"
      },
      "outputs": [],
      "source": [
        "WANDB_PROJ_NAME = \"multilang_asr\"\n",
        "WANDB_EXP_NAME = \"tutorial\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBSDWAtwli0e"
      },
      "outputs": [],
      "source": [
        "# enter your W&B key when prompted\n",
        "wandb_logged_in = wandb.login(relogin=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op9OxsPN111e"
      },
      "source": [
        "The Nemo experiment manager will interface with W&B as well as save your intermediate checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNSmapeTlF6s"
      },
      "outputs": [],
      "source": [
        "from nemo.utils import exp_manager\n",
        "import os\n",
        "\n",
        "config = exp_manager.ExpManagerConfig(\n",
        "    exp_dir=f'experiments/multi/',\n",
        "    name=f\"ASR-Model-multi\",\n",
        "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
        "        monitor=\"val_wer\",\n",
        "        mode=\"min\",\n",
        "        always_save_nemo=True,\n",
        "        save_best_model=True,\n",
        "    ),\n",
        "    create_wandb_logger=wandb_logged_in,\n",
        "    wandb_logger_kwargs = {\"name\": WANDB_EXP_NAME, \"project\": WANDB_PROJ_NAME}\n",
        ")\n",
        "\n",
        "config = OmegaConf.structured(config)\n",
        "\n",
        "logdir = exp_manager.exp_manager(trainer, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDSoypiU111f"
      },
      "source": [
        "## Training\n",
        "Now we start our run with a frozen encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7jf3CdjFMAh"
      },
      "outputs": [],
      "source": [
        "trainer.fit(asr_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otAbsYepBH2O"
      },
      "source": [
        "Let us now test our trained model on the English validation set to measure the WER:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6MBqBfejJb-"
      },
      "outputs": [],
      "source": [
        "test_ds_en = {}\n",
        "test_ds_en['sample_rate'] = 16000\n",
        "test_ds_en['manifest_filepath'] = [val_manifest_en]\n",
        "test_ds_en['batch_size'] = 32\n",
        "test_ds_en['num_workers'] = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K_6XmDKAM6T"
      },
      "outputs": [],
      "source": [
        "asr_model.setup_test_data(test_data_config=test_ds_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ5pdNmBAVa1"
      },
      "outputs": [],
      "source": [
        "trainer.test(asr_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqxL4X3uBNWV"
      },
      "source": [
        "We should get a WER of 15 - 20%.\n",
        "\n",
        "How about Spanish?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krPLVTEFBMpB"
      },
      "outputs": [],
      "source": [
        "test_ds_es = {}\n",
        "test_ds_es['sample_rate'] = 16000\n",
        "test_ds_es['manifest_filepath'] = [val_manifest_es]\n",
        "test_ds_es['batch_size'] = 32\n",
        "test_ds_es['num_workers'] = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6yvTQgPBXWL"
      },
      "outputs": [],
      "source": [
        "asr_model.setup_test_data(test_data_config=test_ds_es)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLL4Uyc9BaOZ"
      },
      "outputs": [],
      "source": [
        "trainer.test(asr_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRHgO3_BftZv"
      },
      "source": [
        "Spanish WER is considerably worse; around 60 - 65% . But, consider that we got this number despite the fact that we are still using a frozen encoder that only saw English speech!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXI3QMbcLOiN"
      },
      "outputs": [],
      "source": [
        "asr_model.save_to(\"multi_trained.nemo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SX3C9zhjNwc"
      },
      "source": [
        "# Next steps\n",
        "In order to achieve better results, try continuing to fine tune from the saved checkpoint using an unfrozen encoder.  You will achieve a much better result if you download full Librispeech english\n",
        "and Mozilla Commonvoice Spanish datasets and finetune on them - you will not need to freeze the encoder due to the larger dataset size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsQmnYdI111j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multilang_asr.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "nteract": {
      "version": "0.28.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "1aaa02ce0ce2638a6e16a203f0ce39bc7495f7236d7115882d2d3541e1318e7a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}